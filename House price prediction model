# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.impute import SimpleImputer

# Load the dataset
data = pd.read_csv('path_to_file.csv')

# Explore the data
print("Dataset shape:", data.shape)
print("First few rows:\n", data.head())
print("Missing values:\n", data.isnull().sum())

# Separate features and target variable
X = data.drop(columns='Price')  # Replace 'Price' with the actual target column name
y = data['Price']  # Target variable

# Separate numerical and categorical columns for preprocessing
num_features = X.select_dtypes(include=['int64', 'float64']).columns
cat_features = X.select_dtypes(include=['object']).columns

# Define preprocessing pipelines
num_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with mean
    ('scaler', StandardScaler())  # Scale features
])

cat_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with most frequent
    ('encoder', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical variables
])

# Combine preprocessing steps
preprocessor = ColumnTransformer([
    ('num', num_pipeline, num_features),
    ('cat', cat_pipeline, cat_features)
])

# Define models to evaluate
models = {
    'Linear Regression': LinearRegression(),
    'Lasso Regression': Lasso(),
    'Ridge Regression': Ridge(),
    'Decision Tree': DecisionTreeRegressor(),
    'Random Forest': RandomForestRegressor()
}

# Function to evaluate models
def evaluate_model(model, X_train, X_test, y_train, y_test):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    return rmse, r2

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a dictionary to store results
results = {}

# Evaluate each model
for model_name, model in models.items():
    pipeline = Pipeline([
        ('preprocessor', preprocessor),
        ('model', model)
    ])
    
    # Evaluate model
    rmse, r2 = evaluate_model(pipeline, X_train, X_test, y_train, y_test)
    results[model_name] = {'RMSE': rmse, 'R^2': r2}
    print(f"{model_name} - RMSE: {rmse:.2f}, R^2: {r2:.2f}")

# Display results
print("\nModel Performance Comparison:")
for model_name, metrics in results.items():
    print(f"{model_name}: RMSE = {metrics['RMSE']:.2f}, R^2 = {metrics['R^2']:.2f}")

# Choose the best model and make predictions on new data if needed
best_model_name = min(results, key=lambda k: results[k]['RMSE'])
print(f"\nBest model: {best_model_name}")
best_model = models[best_model_name]

# Fit best model to entire dataset
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', best_model)
])
pipeline.fit(X, y)

# Predict on new data (replace 'new_data.csv' with the actual file path for new data)
new_data = pd.read_csv('new_data.csv')
predictions = pipeline.predict(new_data)
print("\nPredictions on new data:", predictions)
